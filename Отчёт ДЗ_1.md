Отчет по домашнему заданию №1

ФИО: Маклюкова Алина Александровна

Цель: Освоить работу с тензорами и сравнить производительность CPU и GPU.



1. Работа с тензорами

Тензоры — это основа работы в PyTorch, их можно представить как многомерные массивы. В первой части задания я училась их создавать, менять их форму, а также выполнять базовые операции.
Создание тензоров
Я создала четыре разных тензора:

	- случайные числа от 0 до 1 (размер 3×4),
	
	- тензор нулей (2×3×4),
	
	- тензор единиц (5×5),
	
	- последовательность от 0 до 15, переформатированную в 4×4.

Это помогло понять, как быстро можно сгенерировать нужную структуру данных. 
Пример кода:
torch.rand(3, 4), torch.zeros(2, 3, 4), torch.ones(5, 5), torch.arange(16).reshape(4, 4)


Операции с тензорами

Работала с транспонированием, матричным и поэлементным умножением.
Это ключевые операции при построении моделей.
Главное: нужно строго следить за размерами при умножении тензоров.


Индексация и срезы

Тензоры можно индексировать как обычные многомерные массивы.
Примеры:
	
	tensor[0] — первая строка,
	
	tensor[:, :, -1] — последний столбец,
	
	tensor[2:4,2:4,2:4] — центр,
	
	tensor[::2, ::2, ::2] — только четные индексы.

Это важно для работы с батчами, каналами изображений и т.д.


Изменение формы
Создала тензор из 24 элементов и изменила его форму. Главное, чтобы произведение размеров оставалось тем же. Например:
tensor.reshape(2, 3, 4)




2. Автоматическое дифференцирование
PyTorch позволяет автоматически находить градиенты, это очень важно для обучения нейросетей.


Градиенты сложной функции

Вычисляла производные функции
f(x,y,z)=x2+y2+z2+2xyzf(x, y, z) = x^2 + y^2 + z^2 + 2xyzf(x,y,z)=x2+y2+z2+2xyz
с помощью .backward() — по итогу, всё совпало с ручными расчетами.
Это убедило меня, что autograd действительно выполняет рассчеты безошибочно и корректно.


Градиент функции потерь (MSE)

Реализовала функцию MSE вручную:
Нашла градиенты по w и b. Это показало, как работает обратное распространение ошибки.


Цепное правило

Для функции
f(x)=sin⁡(x2+1)f(x) = sin(x^2 + 1)f(x)=sin(x2+1)
использовала .backward() и autograd.grad, чтобы убедиться, что цепное правило работает правильно. Результаты полностью совпали.



3. CPU vs CUDA: Сравнение производительности
   
Что было сделано

Создала тензоры больших размеров и сравнивала время выполнения операций на CPU и GPU (если доступен).
Операции: матричное умножение, поэлементное сложение и умножение, транспонирование, сумма элементов.

Результаты теста

![1](https://github.com/user-attachments/assets/4e34177b-0f81-4624-b64f-2efca8c32df3)



Выводы:

GPU значительно быстрее, особенно при матричных операциях.
Но если операция простая или объем небольшой, разница минимальна.


Общий вывод
	
	- Я научилась уверенно работать с тензорами: создавать, изменять, индексировать.
	- Освоила автоматическое дифференцирование — это фундамент для обучения моделей.
	- sУбедилась в преимуществах CUDA — GPU ускоряет матричные операции в разы.
